{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam RS - Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.sql.functions import desc, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"spark-recommender\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Import games data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = spark.read.json('./rawdata/apps_detail.json')\n",
    "df_games.printSchema()\n",
    "\n",
    "df_games.createOrReplaceTempView(\"games\")\n",
    "df_games.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play with games data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = spark.sql('''SELECT data.recommendations FROM games''')\n",
    "tmp.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = spark.sql('''SELECT data.categories.id, COUNT(data.categories.id) AS count \\\n",
    "FROM games GROUP BY data.categories.id''')\n",
    "categories.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlGame = spark.sql(\"SELECT appid, data.name FROM games\")\n",
    "sqlGame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = spark.read.json('./rawdata/ratings_detail.json')\n",
    "df_ratings.printSchema()\n",
    "df_ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.distinct()\n",
    "df_ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.show()\n",
    "#### HAVE TO use df.select('col').show() but not df.col.show()\n",
    "df_ratings.select(\"apps\").show()\n",
    "df_ratings.createOrReplaceTempView(\"ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mangle data for ALS input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HOW TO select information in nested JSON:\n",
    "# 1. https://stackoverflow.com/questions/29948789/how-to-parse-nested-json-objects-in-spark-sql\n",
    "# DataFrame app = df.select(\"app\");\n",
    "#        app.printSchema();\n",
    "# DataFrame appName = app.select(\"element.appName\");\n",
    "#        appName.printSchema();\n",
    "# 2. select nested struct with SQL\n",
    "tmp = spark.sql('SELECT apps.lastPlayed FROM ratings')\n",
    "# tmp.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### removing na and add index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove missing value and make an index\n",
    "# Re-organize data structure by map(x:(x.a, x.b))\n",
    "ratings_filtered_rdd = df_ratings.rdd.filter(lambda x: x.apps !=[])\\\n",
    ".map(lambda x: (x.steamID, x.apps))\n",
    "#print(ratings_filtered_rdd.take(1)) #RDD needs print???\n",
    "\n",
    "ratings_filtered_rdd = ratings_filtered_rdd.zipWithIndex()\n",
    "#ratings_filtered_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check information of a given user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "user0 = ratings_filtered_rdd.filter (lambda ((ID, apps), index):index == 0)\n",
    "user0.collect()\n",
    "# user0: [((76561198096934288,\n",
    "#   [Row(appID=u'570', lastPlayed=u'1497026829', name=u'Dota 2', totalTime=u'')]), 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### core of rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract apps information\n",
    "# With index, the extracted info can be easily mapped back to ID\n",
    "training_rdd = ratings_filtered_rdd.map(lambda ((ID, apps), index):(index,apps))\n",
    "\n",
    "# flatMapValues is a combination of flatMap and mapValues\n",
    "# it applies on (key [val] pair), while keeping the keys, flatMap the [val] to each key\n",
    "# https://stackoverflow.com/questions/37302264/spark-flatmapvalues-query\n",
    "training_rdd = training_rdd.flatMapValues(lambda x: x)\n",
    "\n",
    "training_rdd = training_rdd.filter(lambda (x,y): len(y.totalTime) > 0)\n",
    "training_rdd = training_rdd.filter(lambda (x,y): float(y.totalTime.replace(\",\",\"\") > 0))\n",
    "training_rdd = training_rdd.map(lambda (x,y): (x, y.appID, float(y.totalTime.replace(\",\",\"\")))) \n",
    "training_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isFloat(text):\n",
    "    try:\n",
    "        float(text)\n",
    "        return 0\n",
    "    except ValueError:\n",
    "        return text\n",
    "\n",
    "tmp = ratings_filtered_rdd.map(lambda ((ID, apps), index):(index,apps))\\\n",
    ".flatMapValues(lambda x: x)\n",
    "\n",
    "tmp = tmp.map(lambda (x, y): (x, isFloat(y.totalTime.replace(\",\",\"\"))))\n",
    "tmp = tmp.filter(lambda (x, y): (y != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tmp = training_rdd.toDF()\n",
    "#tmp.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training - ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implicit model (vs. explicit model: real 'rating' data\n",
    "# related to the level of confidence in observed user preferences, \n",
    "# rather than explicit ratings given to items. \n",
    "# The model then tries to find latent factors that can be used to \n",
    "# predict the expected preference of a user for an item.\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS\n",
    "model = ALS.trainImplicit(training_rdd, 10)\n",
    "print model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample result display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.recommendProducts(user, product#)\n",
    "try_result_rating = model.recommendProducts(3,5)\n",
    "print try_result_rating\n",
    "print type(try_result_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_result_rating_df = spark.createDataFrame(try_result_rating)\n",
    "try_result_rating_df.sort(desc(\"rating\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_final_result = try_result_rating_df.join(\\\n",
    "                df_games, try_result_rating_df.product == \\\n",
    "                df_games.appid,\"left\")\n",
    "# Or use left join. Left is `try_result_rating_df` here\n",
    "#.select(\"user\",df_games.data.name)\n",
    "try_final_result.show()\n",
    "print df_games.count(), try_result_rating_df.count(), try_final_result.count(), type(try_final_result)\n",
    "try_final_result.select('data').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = try_final_result.select('data')\n",
    "from pyspark.sql.functions import split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate recommended game list for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpID = ratings_filtered_rdd.map(lambda ((ID, apps), index):(index, ID))\n",
    "#tmpID.take(10)\n",
    "\n",
    "# rdd.collectAsMap(): convert tuple to dictionary\n",
    "# type(tmpID_dict): dict\n",
    "tmpID_dict = tmpID.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some user IDs are not in the recommendation system \n",
    "# because they don't have valid game data (totalTime = 0)\n",
    "# Here we extract the userID that are in training data.\n",
    "# This should be done before training steps\n",
    "user_rdd = ratings_filtered_rdd\\\n",
    "           .map(lambda ((ID, apps), index):(index, ID))\n",
    "\n",
    "# http://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=join#pyspark.RDD.join\n",
    "user_rdd = training_rdd\\\n",
    "           .join(user_rdd)\\\n",
    "           .map(lambda (index, (gameID, userID)):(index, userID))\\\n",
    "           .distinct()\n",
    "user_rdd.take(10)\n",
    "user_rdd_dict = user_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rec_list=[]\n",
    "\n",
    "\n",
    "for index in user_rdd_dict.keys():\n",
    "    user_rec_list+=[(user_rdd_dict[index],i.product) for i in model.recommendProducts(index,5)]\n",
    "\n",
    "print user_rec_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot apply SparkContext methods to recommendProducts results\n",
    "# Exception: It appears that you are attempting to reference SparkContext \n",
    "# from a broadcast variable, action, or transformation. \n",
    "# SparkContext can only be used on the driver, not in code that it run on workers. \n",
    "# For more information, see SPARK-5063.\n",
    "\n",
    "#def get_rec_list(x):\n",
    "#    fullList = model.recommendProducts(index,5)\n",
    "    \n",
    "#user_rec_rdd = user_rdd.flatMap(lambda (index, userID) : (userID, [get_rec_list(index)]))\n",
    "#user_rec_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### orgnize and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "          StructField(\"id\", StringType(), True),\n",
    "          StructField(\"game\",IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "\n",
    "result_df = spark.createDataFrame(user_rec_list, schema)\n",
    "result_df.take(3)\n",
    "# EQUIVALENT TO ALL THE LINES BELOW\n",
    "#\n",
    "# all_result_rdd = result_df.rdd.groupByKey().mapValues(list).flatMapValues(lambda x: x)\n",
    "#\n",
    "# .rdd.groupByKey(): \n",
    "# generate (key, <pyspark.resultiterable.ResultIterable at 0x7f8f5e9905d0>)\n",
    "# .rdd.groupByKey().mapValues(list): \n",
    "# (key, [val1, val2, ..])\n",
    "# .rdd.groupByKey().mapValues(list).flatMapValues(lambda x: x)\n",
    "# (key, val1)(key, val2)\n",
    "\n",
    "# dif_all_result=all_result_rdd.toDF(schema)\n",
    "# dif_all_result.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas\n",
    "engine = sqlalchemy.create_engine('sqlite:///game.sqlite3')\n",
    "result_df.toPandas().to_sql('recommended_game', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from jdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to a JDBC source\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlite://game.sqlite3\") \\\n",
    "    .option(\"dbtable\", \"recommended_game\") \\\n",
    "    .option(\"user\", \"\") \\\n",
    "    .option(\"password\", \"\") \\\n",
    "    .load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
